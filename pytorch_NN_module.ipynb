{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a Model class using nn.Module\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, n_features:int):\n",
    "        super(Model1, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_data:torch.Tensor):\n",
    "        output = self.linear(x_data)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def loss_function(self, y_pred:torch.Tensor, y_true:torch.Tensor):\n",
    "        loss = -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "lr = 0.0001\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a model object\n",
    "model1 = Model1(5)\n",
    "\n",
    "# visualizing the model1\n",
    "from torchinfo import summary\n",
    "summary(model1, (10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.6106184720993042\n",
      "Epoch 2/200, Loss: 0.8215181231498718\n",
      "Epoch 3/200, Loss: 0.5587369203567505\n",
      "Epoch 4/200, Loss: 0.7041952610015869\n",
      "Epoch 5/200, Loss: 0.6598166227340698\n",
      "Epoch 6/200, Loss: 0.8282955288887024\n",
      "Epoch 7/200, Loss: 0.7361363172531128\n",
      "Epoch 8/200, Loss: 0.842820942401886\n",
      "Epoch 9/200, Loss: 0.7766400575637817\n",
      "Epoch 10/200, Loss: 0.6741196513175964\n",
      "Epoch 11/200, Loss: 0.8299275636672974\n",
      "Epoch 12/200, Loss: 0.6906008124351501\n",
      "Epoch 13/200, Loss: 0.834429144859314\n",
      "Epoch 14/200, Loss: 0.8763753175735474\n",
      "Epoch 15/200, Loss: 0.6943773031234741\n",
      "Epoch 16/200, Loss: 0.8182080388069153\n",
      "Epoch 17/200, Loss: 0.8093549609184265\n",
      "Epoch 18/200, Loss: 0.7450391054153442\n",
      "Epoch 19/200, Loss: 0.8333975672721863\n",
      "Epoch 20/200, Loss: 0.6886350512504578\n",
      "Epoch 21/200, Loss: 0.6920737624168396\n",
      "Epoch 22/200, Loss: 0.8385375142097473\n",
      "Epoch 23/200, Loss: 0.712393045425415\n",
      "Epoch 24/200, Loss: 0.7979110479354858\n",
      "Epoch 25/200, Loss: 0.7161873579025269\n",
      "Epoch 26/200, Loss: 0.7399846315383911\n",
      "Epoch 27/200, Loss: 0.618058979511261\n",
      "Epoch 28/200, Loss: 0.6461071372032166\n",
      "Epoch 29/200, Loss: 0.7860503792762756\n",
      "Epoch 30/200, Loss: 0.7540863752365112\n",
      "Epoch 31/200, Loss: 0.5247149467468262\n",
      "Epoch 32/200, Loss: 0.6909014582633972\n",
      "Epoch 33/200, Loss: 0.6830926537513733\n",
      "Epoch 34/200, Loss: 0.7413294315338135\n",
      "Epoch 35/200, Loss: 0.830767035484314\n",
      "Epoch 36/200, Loss: 0.7610486745834351\n",
      "Epoch 37/200, Loss: 0.5682979226112366\n",
      "Epoch 38/200, Loss: 0.6778004169464111\n",
      "Epoch 39/200, Loss: 0.8255308270454407\n",
      "Epoch 40/200, Loss: 0.5781925320625305\n",
      "Epoch 41/200, Loss: 0.7914345860481262\n",
      "Epoch 42/200, Loss: 0.6178922653198242\n",
      "Epoch 43/200, Loss: 0.7668644785881042\n",
      "Epoch 44/200, Loss: 0.6576703190803528\n",
      "Epoch 45/200, Loss: 0.6817659139633179\n",
      "Epoch 46/200, Loss: 0.8531578183174133\n",
      "Epoch 47/200, Loss: 0.853845477104187\n",
      "Epoch 48/200, Loss: 0.6764148473739624\n",
      "Epoch 49/200, Loss: 0.6989324688911438\n",
      "Epoch 50/200, Loss: 0.6558032035827637\n",
      "Epoch 51/200, Loss: 0.5651038885116577\n",
      "Epoch 52/200, Loss: 0.8137962222099304\n",
      "Epoch 53/200, Loss: 0.6216450929641724\n",
      "Epoch 54/200, Loss: 0.7896487712860107\n",
      "Epoch 55/200, Loss: 0.6982446908950806\n",
      "Epoch 56/200, Loss: 0.8447235226631165\n",
      "Epoch 57/200, Loss: 0.7685441374778748\n",
      "Epoch 58/200, Loss: 0.777629554271698\n",
      "Epoch 59/200, Loss: 0.6699935793876648\n",
      "Epoch 60/200, Loss: 0.8132463693618774\n",
      "Epoch 61/200, Loss: 0.7075053453445435\n",
      "Epoch 62/200, Loss: 0.6236710548400879\n",
      "Epoch 63/200, Loss: 0.7491946220397949\n",
      "Epoch 64/200, Loss: 0.6469714641571045\n",
      "Epoch 65/200, Loss: 0.6535496115684509\n",
      "Epoch 66/200, Loss: 0.7972513437271118\n",
      "Epoch 67/200, Loss: 0.5451492667198181\n",
      "Epoch 68/200, Loss: 0.6556190252304077\n",
      "Epoch 69/200, Loss: 0.6055250763893127\n",
      "Epoch 70/200, Loss: 0.8127845525741577\n",
      "Epoch 71/200, Loss: 0.6918998956680298\n",
      "Epoch 72/200, Loss: 0.8288854360580444\n",
      "Epoch 73/200, Loss: 0.5941005945205688\n",
      "Epoch 74/200, Loss: 0.7557385563850403\n",
      "Epoch 75/200, Loss: 0.7037423849105835\n",
      "Epoch 76/200, Loss: 0.6876498460769653\n",
      "Epoch 77/200, Loss: 0.7150431871414185\n",
      "Epoch 78/200, Loss: 0.8736284375190735\n",
      "Epoch 79/200, Loss: 0.6627291440963745\n",
      "Epoch 80/200, Loss: 0.5965412259101868\n",
      "Epoch 81/200, Loss: 0.6423649787902832\n",
      "Epoch 82/200, Loss: 0.8597590327262878\n",
      "Epoch 83/200, Loss: 0.6201034784317017\n",
      "Epoch 84/200, Loss: 0.8866686820983887\n",
      "Epoch 85/200, Loss: 0.874902606010437\n",
      "Epoch 86/200, Loss: 0.659841001033783\n",
      "Epoch 87/200, Loss: 0.8807266354560852\n",
      "Epoch 88/200, Loss: 0.9275722503662109\n",
      "Epoch 89/200, Loss: 0.7743152379989624\n",
      "Epoch 90/200, Loss: 0.769912600517273\n",
      "Epoch 91/200, Loss: 0.6656904816627502\n",
      "Epoch 92/200, Loss: 0.6959520578384399\n",
      "Epoch 93/200, Loss: 0.816706657409668\n",
      "Epoch 94/200, Loss: 0.7491074800491333\n",
      "Epoch 95/200, Loss: 0.6632636785507202\n",
      "Epoch 96/200, Loss: 0.7509085536003113\n",
      "Epoch 97/200, Loss: 0.6399222016334534\n",
      "Epoch 98/200, Loss: 0.6755260229110718\n",
      "Epoch 99/200, Loss: 0.6485812664031982\n",
      "Epoch 100/200, Loss: 0.7788516283035278\n",
      "Epoch 101/200, Loss: 0.8086167573928833\n",
      "Epoch 102/200, Loss: 0.6748896241188049\n",
      "Epoch 103/200, Loss: 0.7084929943084717\n",
      "Epoch 104/200, Loss: 0.7526276707649231\n",
      "Epoch 105/200, Loss: 0.8333632349967957\n",
      "Epoch 106/200, Loss: 0.6574383974075317\n",
      "Epoch 107/200, Loss: 0.6375239491462708\n",
      "Epoch 108/200, Loss: 0.7699625492095947\n",
      "Epoch 109/200, Loss: 0.7565902471542358\n",
      "Epoch 110/200, Loss: 0.786116361618042\n",
      "Epoch 111/200, Loss: 0.7703021764755249\n",
      "Epoch 112/200, Loss: 0.6616086363792419\n",
      "Epoch 113/200, Loss: 0.701019823551178\n",
      "Epoch 114/200, Loss: 0.7123003602027893\n",
      "Epoch 115/200, Loss: 0.7220391631126404\n",
      "Epoch 116/200, Loss: 0.734204888343811\n",
      "Epoch 117/200, Loss: 0.64608234167099\n",
      "Epoch 118/200, Loss: 0.7913453578948975\n",
      "Epoch 119/200, Loss: 0.7211233973503113\n",
      "Epoch 120/200, Loss: 0.6168521046638489\n",
      "Epoch 121/200, Loss: 0.8332627415657043\n",
      "Epoch 122/200, Loss: 0.8124971389770508\n",
      "Epoch 123/200, Loss: 0.7592596411705017\n",
      "Epoch 124/200, Loss: 0.8785102963447571\n",
      "Epoch 125/200, Loss: 0.9003828763961792\n",
      "Epoch 126/200, Loss: 0.7118019461631775\n",
      "Epoch 127/200, Loss: 0.6985520124435425\n",
      "Epoch 128/200, Loss: 0.7121781706809998\n",
      "Epoch 129/200, Loss: 0.6462720036506653\n",
      "Epoch 130/200, Loss: 0.8077859878540039\n",
      "Epoch 131/200, Loss: 0.6798698902130127\n",
      "Epoch 132/200, Loss: 0.7670767903327942\n",
      "Epoch 133/200, Loss: 0.7510119676589966\n",
      "Epoch 134/200, Loss: 0.9234803915023804\n",
      "Epoch 135/200, Loss: 0.6595114469528198\n",
      "Epoch 136/200, Loss: 0.7723636031150818\n",
      "Epoch 137/200, Loss: 0.8724936246871948\n",
      "Epoch 138/200, Loss: 0.7597999572753906\n",
      "Epoch 139/200, Loss: 0.6921677589416504\n",
      "Epoch 140/200, Loss: 0.619414210319519\n",
      "Epoch 141/200, Loss: 0.9236282110214233\n",
      "Epoch 142/200, Loss: 0.6358529925346375\n",
      "Epoch 143/200, Loss: 0.8231557011604309\n",
      "Epoch 144/200, Loss: 0.7420288324356079\n",
      "Epoch 145/200, Loss: 0.6593493223190308\n",
      "Epoch 146/200, Loss: 0.7727726101875305\n",
      "Epoch 147/200, Loss: 0.8273131251335144\n",
      "Epoch 148/200, Loss: 0.8168733716011047\n",
      "Epoch 149/200, Loss: 0.7167407274246216\n",
      "Epoch 150/200, Loss: 0.539450466632843\n",
      "Epoch 151/200, Loss: 0.7820426821708679\n",
      "Epoch 152/200, Loss: 0.7270117998123169\n",
      "Epoch 153/200, Loss: 0.6518567204475403\n",
      "Epoch 154/200, Loss: 0.6419421434402466\n",
      "Epoch 155/200, Loss: 0.6681925058364868\n",
      "Epoch 156/200, Loss: 0.8285687565803528\n",
      "Epoch 157/200, Loss: 0.7380362749099731\n",
      "Epoch 158/200, Loss: 0.797148585319519\n",
      "Epoch 159/200, Loss: 0.7580457925796509\n",
      "Epoch 160/200, Loss: 0.865157425403595\n",
      "Epoch 161/200, Loss: 0.7167807817459106\n",
      "Epoch 162/200, Loss: 0.949263870716095\n",
      "Epoch 163/200, Loss: 0.7325962781906128\n",
      "Epoch 164/200, Loss: 0.7378134727478027\n",
      "Epoch 165/200, Loss: 0.7529272437095642\n",
      "Epoch 166/200, Loss: 0.6777616739273071\n",
      "Epoch 167/200, Loss: 0.7603248357772827\n",
      "Epoch 168/200, Loss: 0.7824853658676147\n",
      "Epoch 169/200, Loss: 0.8787447810173035\n",
      "Epoch 170/200, Loss: 0.7452294230461121\n",
      "Epoch 171/200, Loss: 0.7096129655838013\n",
      "Epoch 172/200, Loss: 0.6777783632278442\n",
      "Epoch 173/200, Loss: 0.7277990579605103\n",
      "Epoch 174/200, Loss: 0.6824495196342468\n",
      "Epoch 175/200, Loss: 0.7059258818626404\n",
      "Epoch 176/200, Loss: 0.6524469256401062\n",
      "Epoch 177/200, Loss: 0.6901117563247681\n",
      "Epoch 178/200, Loss: 0.705565869808197\n",
      "Epoch 179/200, Loss: 0.6773999333381653\n",
      "Epoch 180/200, Loss: 0.749658465385437\n",
      "Epoch 181/200, Loss: 0.6916722655296326\n",
      "Epoch 182/200, Loss: 0.8594002723693848\n",
      "Epoch 183/200, Loss: 0.4877575933933258\n",
      "Epoch 184/200, Loss: 0.7101680040359497\n",
      "Epoch 185/200, Loss: 0.7762120366096497\n",
      "Epoch 186/200, Loss: 0.7386502027511597\n",
      "Epoch 187/200, Loss: 0.5507082939147949\n",
      "Epoch 188/200, Loss: 0.6284269690513611\n",
      "Epoch 189/200, Loss: 0.6883703470230103\n",
      "Epoch 190/200, Loss: 0.5992265939712524\n",
      "Epoch 191/200, Loss: 0.7314060926437378\n",
      "Epoch 192/200, Loss: 0.7803894877433777\n",
      "Epoch 193/200, Loss: 0.6770354509353638\n",
      "Epoch 194/200, Loss: 0.6596129536628723\n",
      "Epoch 195/200, Loss: 0.700400173664093\n",
      "Epoch 196/200, Loss: 0.7713173031806946\n",
      "Epoch 197/200, Loss: 0.7243949770927429\n",
      "Epoch 198/200, Loss: 0.8505291938781738\n",
      "Epoch 199/200, Loss: 0.7092133164405823\n",
      "Epoch 200/200, Loss: 0.6737853288650513\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # forward pass\n",
    "    y_pred = model1(torch.randn(10, 5))\n",
    "    y_true = torch.randint(0, 2, (10, 1)).float()\n",
    "    loss = model1.loss_function(y_pred, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        model1.linear.weight -= lr * model1.linear.weight.grad\n",
    "        model1.linear.bias -= lr * model1.linear.bias.grad\n",
    "    \n",
    "    # zero the gradients\n",
    "    model1.linear.weight.grad.zero_()\n",
    "    model1.linear.bias.grad.zero_()\n",
    "    \n",
    "    # print the loss\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model2 using nn.Sequential\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, n_features:int):\n",
    "        super(Model2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_features, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_data:torch.Tensor):\n",
    "        return self.model(x_data)\n",
    "    \n",
    "    def loss_function(self, y_pred:torch.Tensor, y_true:torch.Tensor):\n",
    "        loss = -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "lr = 0.0001\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model2                                   [10, 1]                   --\n",
       "├─Sequential: 1-1                        [10, 1]                   --\n",
       "│    └─Linear: 2-1                       [10, 1]                   6\n",
       "│    └─Sigmoid: 2-2                      [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 6\n",
       "Trainable params: 6\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining model2\n",
    "\n",
    "model2 = Model2(5)\n",
    "\n",
    "# visualizing the model2\n",
    "summary(model2, (10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.8746217489242554\n",
      "Epoch 2/200, Loss: 0.7460437417030334\n",
      "Epoch 3/200, Loss: 0.48845285177230835\n",
      "Epoch 4/200, Loss: 0.6233662366867065\n",
      "Epoch 5/200, Loss: 0.82603520154953\n",
      "Epoch 6/200, Loss: 0.6628960371017456\n",
      "Epoch 7/200, Loss: 0.655364453792572\n",
      "Epoch 8/200, Loss: 0.7911667227745056\n",
      "Epoch 9/200, Loss: 0.6499820947647095\n",
      "Epoch 10/200, Loss: 0.6998913884162903\n",
      "Epoch 11/200, Loss: 0.6118136644363403\n",
      "Epoch 12/200, Loss: 0.5665348768234253\n",
      "Epoch 13/200, Loss: 0.8667082786560059\n",
      "Epoch 14/200, Loss: 0.7214802503585815\n",
      "Epoch 15/200, Loss: 0.5824559926986694\n",
      "Epoch 16/200, Loss: 0.8870245218276978\n",
      "Epoch 17/200, Loss: 0.7382663488388062\n",
      "Epoch 18/200, Loss: 0.8707872629165649\n",
      "Epoch 19/200, Loss: 0.6221039295196533\n",
      "Epoch 20/200, Loss: 0.862585186958313\n",
      "Epoch 21/200, Loss: 0.647745668888092\n",
      "Epoch 22/200, Loss: 0.9202569127082825\n",
      "Epoch 23/200, Loss: 0.8639024496078491\n",
      "Epoch 24/200, Loss: 0.7704904079437256\n",
      "Epoch 25/200, Loss: 0.6340122818946838\n",
      "Epoch 26/200, Loss: 0.6876745223999023\n",
      "Epoch 27/200, Loss: 0.8084866404533386\n",
      "Epoch 28/200, Loss: 0.8205552101135254\n",
      "Epoch 29/200, Loss: 0.6694513559341431\n",
      "Epoch 30/200, Loss: 0.7235906720161438\n",
      "Epoch 31/200, Loss: 0.7813596725463867\n",
      "Epoch 32/200, Loss: 0.681065559387207\n",
      "Epoch 33/200, Loss: 0.676032304763794\n",
      "Epoch 34/200, Loss: 0.6116942167282104\n",
      "Epoch 35/200, Loss: 0.6703823804855347\n",
      "Epoch 36/200, Loss: 0.7577376365661621\n",
      "Epoch 37/200, Loss: 0.8619414567947388\n",
      "Epoch 38/200, Loss: 0.7833877801895142\n",
      "Epoch 39/200, Loss: 0.7276801466941833\n",
      "Epoch 40/200, Loss: 0.79274582862854\n",
      "Epoch 41/200, Loss: 0.8505892753601074\n",
      "Epoch 42/200, Loss: 0.7051664590835571\n",
      "Epoch 43/200, Loss: 0.7633435130119324\n",
      "Epoch 44/200, Loss: 0.7497366070747375\n",
      "Epoch 45/200, Loss: 0.5630900859832764\n",
      "Epoch 46/200, Loss: 0.7795106172561646\n",
      "Epoch 47/200, Loss: 0.9757919311523438\n",
      "Epoch 48/200, Loss: 0.7317224144935608\n",
      "Epoch 49/200, Loss: 0.8320094347000122\n",
      "Epoch 50/200, Loss: 0.6631521582603455\n",
      "Epoch 51/200, Loss: 0.7666579484939575\n",
      "Epoch 52/200, Loss: 0.7542446851730347\n",
      "Epoch 53/200, Loss: 0.8074520826339722\n",
      "Epoch 54/200, Loss: 0.7502515316009521\n",
      "Epoch 55/200, Loss: 0.6827265024185181\n",
      "Epoch 56/200, Loss: 0.6160596609115601\n",
      "Epoch 57/200, Loss: 0.6548158526420593\n",
      "Epoch 58/200, Loss: 0.7970504760742188\n",
      "Epoch 59/200, Loss: 0.723235547542572\n",
      "Epoch 60/200, Loss: 0.6219310760498047\n",
      "Epoch 61/200, Loss: 0.5917277336120605\n",
      "Epoch 62/200, Loss: 0.5881329774856567\n",
      "Epoch 63/200, Loss: 0.715492844581604\n",
      "Epoch 64/200, Loss: 0.786493182182312\n",
      "Epoch 65/200, Loss: 0.6025588512420654\n",
      "Epoch 66/200, Loss: 0.9549992680549622\n",
      "Epoch 67/200, Loss: 0.761803925037384\n",
      "Epoch 68/200, Loss: 0.7587426900863647\n",
      "Epoch 69/200, Loss: 0.7636300325393677\n",
      "Epoch 70/200, Loss: 0.7589371204376221\n",
      "Epoch 71/200, Loss: 0.6047607660293579\n",
      "Epoch 72/200, Loss: 0.8275465965270996\n",
      "Epoch 73/200, Loss: 0.877307116985321\n",
      "Epoch 74/200, Loss: 0.7249621748924255\n",
      "Epoch 75/200, Loss: 0.9182338714599609\n",
      "Epoch 76/200, Loss: 0.7433785200119019\n",
      "Epoch 77/200, Loss: 0.8618451952934265\n",
      "Epoch 78/200, Loss: 0.6797233819961548\n",
      "Epoch 79/200, Loss: 0.6610860228538513\n",
      "Epoch 80/200, Loss: 0.6487871408462524\n",
      "Epoch 81/200, Loss: 0.8150647878646851\n",
      "Epoch 82/200, Loss: 0.8541316986083984\n",
      "Epoch 83/200, Loss: 0.6893240213394165\n",
      "Epoch 84/200, Loss: 0.6701393127441406\n",
      "Epoch 85/200, Loss: 0.746044397354126\n",
      "Epoch 86/200, Loss: 0.7829781174659729\n",
      "Epoch 87/200, Loss: 0.6838955879211426\n",
      "Epoch 88/200, Loss: 0.6630003452301025\n",
      "Epoch 89/200, Loss: 0.6936460733413696\n",
      "Epoch 90/200, Loss: 0.7944976091384888\n",
      "Epoch 91/200, Loss: 0.7663605809211731\n",
      "Epoch 92/200, Loss: 0.8069900274276733\n",
      "Epoch 93/200, Loss: 0.8964700698852539\n",
      "Epoch 94/200, Loss: 0.8028284907341003\n",
      "Epoch 95/200, Loss: 0.6653280258178711\n",
      "Epoch 96/200, Loss: 0.7994953989982605\n",
      "Epoch 97/200, Loss: 0.8613883256912231\n",
      "Epoch 98/200, Loss: 0.8042348027229309\n",
      "Epoch 99/200, Loss: 0.8317810297012329\n",
      "Epoch 100/200, Loss: 0.8291373252868652\n",
      "Epoch 101/200, Loss: 0.9737434387207031\n",
      "Epoch 102/200, Loss: 0.7417784333229065\n",
      "Epoch 103/200, Loss: 0.6965228319168091\n",
      "Epoch 104/200, Loss: 0.7940134406089783\n",
      "Epoch 105/200, Loss: 0.6878843307495117\n",
      "Epoch 106/200, Loss: 0.7736215591430664\n",
      "Epoch 107/200, Loss: 0.6203665733337402\n",
      "Epoch 108/200, Loss: 0.7203069925308228\n",
      "Epoch 109/200, Loss: 0.8095958828926086\n",
      "Epoch 110/200, Loss: 0.7671011686325073\n",
      "Epoch 111/200, Loss: 0.7376165390014648\n",
      "Epoch 112/200, Loss: 0.7547575235366821\n",
      "Epoch 113/200, Loss: 0.6567114591598511\n",
      "Epoch 114/200, Loss: 0.7234504818916321\n",
      "Epoch 115/200, Loss: 0.8899484872817993\n",
      "Epoch 116/200, Loss: 0.6834796667098999\n",
      "Epoch 117/200, Loss: 0.8096153140068054\n",
      "Epoch 118/200, Loss: 0.7300837635993958\n",
      "Epoch 119/200, Loss: 0.7566888928413391\n",
      "Epoch 120/200, Loss: 0.7814292311668396\n",
      "Epoch 121/200, Loss: 0.8245083689689636\n",
      "Epoch 122/200, Loss: 0.6039801239967346\n",
      "Epoch 123/200, Loss: 0.6869405508041382\n",
      "Epoch 124/200, Loss: 0.6409556865692139\n",
      "Epoch 125/200, Loss: 0.63692307472229\n",
      "Epoch 126/200, Loss: 0.758023202419281\n",
      "Epoch 127/200, Loss: 0.7473632097244263\n",
      "Epoch 128/200, Loss: 0.6458247303962708\n",
      "Epoch 129/200, Loss: 0.8484787940979004\n",
      "Epoch 130/200, Loss: 0.5618695020675659\n",
      "Epoch 131/200, Loss: 0.8927055597305298\n",
      "Epoch 132/200, Loss: 0.6573279500007629\n",
      "Epoch 133/200, Loss: 0.6346380710601807\n",
      "Epoch 134/200, Loss: 0.847344696521759\n",
      "Epoch 135/200, Loss: 0.7041856050491333\n",
      "Epoch 136/200, Loss: 0.6806509494781494\n",
      "Epoch 137/200, Loss: 0.6805518865585327\n",
      "Epoch 138/200, Loss: 0.887736976146698\n",
      "Epoch 139/200, Loss: 0.5757750868797302\n",
      "Epoch 140/200, Loss: 0.7911295294761658\n",
      "Epoch 141/200, Loss: 0.6837552785873413\n",
      "Epoch 142/200, Loss: 0.6989904046058655\n",
      "Epoch 143/200, Loss: 0.7786432504653931\n",
      "Epoch 144/200, Loss: 0.8323237299919128\n",
      "Epoch 145/200, Loss: 0.7177642583847046\n",
      "Epoch 146/200, Loss: 0.6719247102737427\n",
      "Epoch 147/200, Loss: 0.8102988004684448\n",
      "Epoch 148/200, Loss: 0.6848418116569519\n",
      "Epoch 149/200, Loss: 0.8543744087219238\n",
      "Epoch 150/200, Loss: 0.6884917616844177\n",
      "Epoch 151/200, Loss: 0.7505617141723633\n",
      "Epoch 152/200, Loss: 0.7247756719589233\n",
      "Epoch 153/200, Loss: 0.7500316500663757\n",
      "Epoch 154/200, Loss: 0.8252048492431641\n",
      "Epoch 155/200, Loss: 0.6265503764152527\n",
      "Epoch 156/200, Loss: 0.9098244905471802\n",
      "Epoch 157/200, Loss: 0.6998761296272278\n",
      "Epoch 158/200, Loss: 0.6385020017623901\n",
      "Epoch 159/200, Loss: 0.7338332533836365\n",
      "Epoch 160/200, Loss: 0.7580929398536682\n",
      "Epoch 161/200, Loss: 0.8596086502075195\n",
      "Epoch 162/200, Loss: 0.7114289999008179\n",
      "Epoch 163/200, Loss: 0.8578904867172241\n",
      "Epoch 164/200, Loss: 0.8234508633613586\n",
      "Epoch 165/200, Loss: 0.7784098386764526\n",
      "Epoch 166/200, Loss: 0.5884727239608765\n",
      "Epoch 167/200, Loss: 0.6788159012794495\n",
      "Epoch 168/200, Loss: 0.6958497166633606\n",
      "Epoch 169/200, Loss: 0.7948166131973267\n",
      "Epoch 170/200, Loss: 0.7080885171890259\n",
      "Epoch 171/200, Loss: 0.7271898984909058\n",
      "Epoch 172/200, Loss: 0.9428147077560425\n",
      "Epoch 173/200, Loss: 0.6020945310592651\n",
      "Epoch 174/200, Loss: 0.6373528242111206\n",
      "Epoch 175/200, Loss: 0.6533266305923462\n",
      "Epoch 176/200, Loss: 0.8330281376838684\n",
      "Epoch 177/200, Loss: 0.7619041204452515\n",
      "Epoch 178/200, Loss: 0.6630136370658875\n",
      "Epoch 179/200, Loss: 1.086777925491333\n",
      "Epoch 180/200, Loss: 0.6212250590324402\n",
      "Epoch 181/200, Loss: 0.6330952644348145\n",
      "Epoch 182/200, Loss: 0.8350822329521179\n",
      "Epoch 183/200, Loss: 0.7654479742050171\n",
      "Epoch 184/200, Loss: 0.7812032103538513\n",
      "Epoch 185/200, Loss: 0.7146148681640625\n",
      "Epoch 186/200, Loss: 0.7333806753158569\n",
      "Epoch 187/200, Loss: 0.7074311971664429\n",
      "Epoch 188/200, Loss: 0.8165099024772644\n",
      "Epoch 189/200, Loss: 0.7372905611991882\n",
      "Epoch 190/200, Loss: 0.6960344314575195\n",
      "Epoch 191/200, Loss: 0.8550551533699036\n",
      "Epoch 192/200, Loss: 0.9126102328300476\n",
      "Epoch 193/200, Loss: 0.7822974920272827\n",
      "Epoch 194/200, Loss: 0.6806622743606567\n",
      "Epoch 195/200, Loss: 0.7046929597854614\n",
      "Epoch 196/200, Loss: 0.8194171190261841\n",
      "Epoch 197/200, Loss: 0.709587037563324\n",
      "Epoch 198/200, Loss: 0.8875102996826172\n",
      "Epoch 199/200, Loss: 0.5717484951019287\n",
      "Epoch 200/200, Loss: 0.8060417175292969\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # forward pass\n",
    "    y_pred = model2(torch.randn(10, 5))\n",
    "    y_true = torch.randint(0, 2, (10, 1)).float()\n",
    "    \n",
    "    # calculating the loss\n",
    "    loss = model2.loss_function(y_pred, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # updating the weights and biases\n",
    "    with torch.no_grad():\n",
    "        model2.model[0].weight -= lr * model2.model[0].weight.grad\n",
    "        model2.model[0].bias -= lr * model2.model[0].bias.grad\n",
    "    \n",
    "    # zero the gradients\n",
    "    model2.model[0].weight.grad.zero_()\n",
    "    model2.model[0].bias.grad.zero_()\n",
    "    \n",
    "    # print the loss\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model using prebuild loss function in torch.nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self, n_features:int):\n",
    "        super(Model3, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_features, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_data:torch.Tensor):\n",
    "        return self.model(x_data)\n",
    "    \n",
    "    def loss_function(self, y_pred:torch.Tensor, y_true:torch.Tensor):\n",
    "        loss = -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definig  model3\n",
    "model3 = Model3(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters\n",
    "lr = 0.0001\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using buildin loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.7234200239181519\n",
      "Epoch 2/200, Loss: 0.6943084597587585\n",
      "Epoch 3/200, Loss: 0.977628231048584\n",
      "Epoch 4/200, Loss: 0.8012145161628723\n",
      "Epoch 5/200, Loss: 0.7539384961128235\n",
      "Epoch 6/200, Loss: 0.5701683163642883\n",
      "Epoch 7/200, Loss: 0.7558839321136475\n",
      "Epoch 8/200, Loss: 0.7729441523551941\n",
      "Epoch 9/200, Loss: 0.7534897923469543\n",
      "Epoch 10/200, Loss: 0.728026270866394\n",
      "Epoch 11/200, Loss: 0.637759804725647\n",
      "Epoch 12/200, Loss: 0.8555766940116882\n",
      "Epoch 13/200, Loss: 0.642876148223877\n",
      "Epoch 14/200, Loss: 1.0159004926681519\n",
      "Epoch 15/200, Loss: 0.6143859624862671\n",
      "Epoch 16/200, Loss: 0.7843916416168213\n",
      "Epoch 17/200, Loss: 0.6661587953567505\n",
      "Epoch 18/200, Loss: 0.7283618450164795\n",
      "Epoch 19/200, Loss: 0.8041883707046509\n",
      "Epoch 20/200, Loss: 0.807680606842041\n",
      "Epoch 21/200, Loss: 0.7659701108932495\n",
      "Epoch 22/200, Loss: 0.8417078256607056\n",
      "Epoch 23/200, Loss: 0.7244749665260315\n",
      "Epoch 24/200, Loss: 0.7250086069107056\n",
      "Epoch 25/200, Loss: 0.8034807443618774\n",
      "Epoch 26/200, Loss: 0.7254595756530762\n",
      "Epoch 27/200, Loss: 0.6954062581062317\n",
      "Epoch 28/200, Loss: 0.7024244666099548\n",
      "Epoch 29/200, Loss: 0.7069827914237976\n",
      "Epoch 30/200, Loss: 0.792799711227417\n",
      "Epoch 31/200, Loss: 0.5900036692619324\n",
      "Epoch 32/200, Loss: 0.8461506962776184\n",
      "Epoch 33/200, Loss: 0.798738420009613\n",
      "Epoch 34/200, Loss: 0.6673978567123413\n",
      "Epoch 35/200, Loss: 0.622918963432312\n",
      "Epoch 36/200, Loss: 0.8598936796188354\n",
      "Epoch 37/200, Loss: 0.7030404806137085\n",
      "Epoch 38/200, Loss: 0.7126848101615906\n",
      "Epoch 39/200, Loss: 0.6826627850532532\n",
      "Epoch 40/200, Loss: 0.7893342971801758\n",
      "Epoch 41/200, Loss: 0.8087800145149231\n",
      "Epoch 42/200, Loss: 0.7698945999145508\n",
      "Epoch 43/200, Loss: 0.6519759297370911\n",
      "Epoch 44/200, Loss: 0.780259370803833\n",
      "Epoch 45/200, Loss: 0.7491723895072937\n",
      "Epoch 46/200, Loss: 0.7162939310073853\n",
      "Epoch 47/200, Loss: 0.8156312704086304\n",
      "Epoch 48/200, Loss: 0.6190358996391296\n",
      "Epoch 49/200, Loss: 0.6006184220314026\n",
      "Epoch 50/200, Loss: 0.8639094233512878\n",
      "Epoch 51/200, Loss: 0.7389568090438843\n",
      "Epoch 52/200, Loss: 0.7275275588035583\n",
      "Epoch 53/200, Loss: 0.7827793955802917\n",
      "Epoch 54/200, Loss: 0.7140008807182312\n",
      "Epoch 55/200, Loss: 0.9477605819702148\n",
      "Epoch 56/200, Loss: 0.606291651725769\n",
      "Epoch 57/200, Loss: 0.7216911315917969\n",
      "Epoch 58/200, Loss: 0.8614099621772766\n",
      "Epoch 59/200, Loss: 0.7192447185516357\n",
      "Epoch 60/200, Loss: 0.791965901851654\n",
      "Epoch 61/200, Loss: 0.6530641317367554\n",
      "Epoch 62/200, Loss: 0.6668818593025208\n",
      "Epoch 63/200, Loss: 0.7449930310249329\n",
      "Epoch 64/200, Loss: 0.6924872994422913\n",
      "Epoch 65/200, Loss: 0.8358203768730164\n",
      "Epoch 66/200, Loss: 0.6666627526283264\n",
      "Epoch 67/200, Loss: 0.6345365047454834\n",
      "Epoch 68/200, Loss: 0.8045473098754883\n",
      "Epoch 69/200, Loss: 0.8613074421882629\n",
      "Epoch 70/200, Loss: 0.6629058122634888\n",
      "Epoch 71/200, Loss: 0.7766097784042358\n",
      "Epoch 72/200, Loss: 0.6415005922317505\n",
      "Epoch 73/200, Loss: 0.6665562391281128\n",
      "Epoch 74/200, Loss: 0.644020140171051\n",
      "Epoch 75/200, Loss: 0.8171855807304382\n",
      "Epoch 76/200, Loss: 0.5794306993484497\n",
      "Epoch 77/200, Loss: 0.5842893719673157\n",
      "Epoch 78/200, Loss: 0.6445327997207642\n",
      "Epoch 79/200, Loss: 0.7101252675056458\n",
      "Epoch 80/200, Loss: 0.7897385358810425\n",
      "Epoch 81/200, Loss: 0.8645588159561157\n",
      "Epoch 82/200, Loss: 0.7525447607040405\n",
      "Epoch 83/200, Loss: 0.7570408582687378\n",
      "Epoch 84/200, Loss: 0.8408464193344116\n",
      "Epoch 85/200, Loss: 0.6339700818061829\n",
      "Epoch 86/200, Loss: 0.6299105286598206\n",
      "Epoch 87/200, Loss: 0.8136040568351746\n",
      "Epoch 88/200, Loss: 0.7347462177276611\n",
      "Epoch 89/200, Loss: 0.7658411264419556\n",
      "Epoch 90/200, Loss: 0.6842886805534363\n",
      "Epoch 91/200, Loss: 0.687167763710022\n",
      "Epoch 92/200, Loss: 0.6339896321296692\n",
      "Epoch 93/200, Loss: 0.798578143119812\n",
      "Epoch 94/200, Loss: 0.7579110860824585\n",
      "Epoch 95/200, Loss: 0.7115791440010071\n",
      "Epoch 96/200, Loss: 0.6743764281272888\n",
      "Epoch 97/200, Loss: 0.7217767238616943\n",
      "Epoch 98/200, Loss: 0.8429745435714722\n",
      "Epoch 99/200, Loss: 0.6181694269180298\n",
      "Epoch 100/200, Loss: 0.7013753652572632\n",
      "Epoch 101/200, Loss: 0.7971429824829102\n",
      "Epoch 102/200, Loss: 0.5820153951644897\n",
      "Epoch 103/200, Loss: 0.6290217638015747\n",
      "Epoch 104/200, Loss: 0.7458223104476929\n",
      "Epoch 105/200, Loss: 0.6649288535118103\n",
      "Epoch 106/200, Loss: 0.5628134608268738\n",
      "Epoch 107/200, Loss: 0.8313614726066589\n",
      "Epoch 108/200, Loss: 0.834023118019104\n",
      "Epoch 109/200, Loss: 0.734005868434906\n",
      "Epoch 110/200, Loss: 0.7261484265327454\n",
      "Epoch 111/200, Loss: 0.8897095918655396\n",
      "Epoch 112/200, Loss: 0.7552132606506348\n",
      "Epoch 113/200, Loss: 0.78520268201828\n",
      "Epoch 114/200, Loss: 0.9460732340812683\n",
      "Epoch 115/200, Loss: 0.8004764318466187\n",
      "Epoch 116/200, Loss: 0.7603260278701782\n",
      "Epoch 117/200, Loss: 0.7012564539909363\n",
      "Epoch 118/200, Loss: 0.6737496852874756\n",
      "Epoch 119/200, Loss: 0.8185393214225769\n",
      "Epoch 120/200, Loss: 0.7450883388519287\n",
      "Epoch 121/200, Loss: 0.6982394456863403\n",
      "Epoch 122/200, Loss: 0.750299334526062\n",
      "Epoch 123/200, Loss: 0.776322066783905\n",
      "Epoch 124/200, Loss: 0.6747719049453735\n",
      "Epoch 125/200, Loss: 0.6299909949302673\n",
      "Epoch 126/200, Loss: 0.7276989221572876\n",
      "Epoch 127/200, Loss: 0.8604190945625305\n",
      "Epoch 128/200, Loss: 0.6280227899551392\n",
      "Epoch 129/200, Loss: 0.7157479524612427\n",
      "Epoch 130/200, Loss: 0.7073768377304077\n",
      "Epoch 131/200, Loss: 0.6614004969596863\n",
      "Epoch 132/200, Loss: 0.7155728936195374\n",
      "Epoch 133/200, Loss: 0.7264049649238586\n",
      "Epoch 134/200, Loss: 0.6541265845298767\n",
      "Epoch 135/200, Loss: 0.7389712929725647\n",
      "Epoch 136/200, Loss: 0.6529478430747986\n",
      "Epoch 137/200, Loss: 0.5975163578987122\n",
      "Epoch 138/200, Loss: 0.6994159817695618\n",
      "Epoch 139/200, Loss: 0.7076514363288879\n",
      "Epoch 140/200, Loss: 0.8662256002426147\n",
      "Epoch 141/200, Loss: 0.8283950686454773\n",
      "Epoch 142/200, Loss: 0.9470751881599426\n",
      "Epoch 143/200, Loss: 0.7875310182571411\n",
      "Epoch 144/200, Loss: 0.6950953006744385\n",
      "Epoch 145/200, Loss: 0.7142156958580017\n",
      "Epoch 146/200, Loss: 0.7356796264648438\n",
      "Epoch 147/200, Loss: 0.8352702260017395\n",
      "Epoch 148/200, Loss: 0.6044374704360962\n",
      "Epoch 149/200, Loss: 0.7955608367919922\n",
      "Epoch 150/200, Loss: 0.7579604983329773\n",
      "Epoch 151/200, Loss: 0.7563740611076355\n",
      "Epoch 152/200, Loss: 0.6132978200912476\n",
      "Epoch 153/200, Loss: 0.7310382127761841\n",
      "Epoch 154/200, Loss: 0.6429985761642456\n",
      "Epoch 155/200, Loss: 0.6982396841049194\n",
      "Epoch 156/200, Loss: 0.6462548971176147\n",
      "Epoch 157/200, Loss: 0.8453986048698425\n",
      "Epoch 158/200, Loss: 0.6323366761207581\n",
      "Epoch 159/200, Loss: 0.788249671459198\n",
      "Epoch 160/200, Loss: 0.6780547499656677\n",
      "Epoch 161/200, Loss: 0.6655335426330566\n",
      "Epoch 162/200, Loss: 0.7197653651237488\n",
      "Epoch 163/200, Loss: 0.7410266399383545\n",
      "Epoch 164/200, Loss: 0.8327111005783081\n",
      "Epoch 165/200, Loss: 0.7041220664978027\n",
      "Epoch 166/200, Loss: 0.5527255535125732\n",
      "Epoch 167/200, Loss: 0.7056621313095093\n",
      "Epoch 168/200, Loss: 0.6720260381698608\n",
      "Epoch 169/200, Loss: 0.8513256907463074\n",
      "Epoch 170/200, Loss: 0.6460158228874207\n",
      "Epoch 171/200, Loss: 0.6279019117355347\n",
      "Epoch 172/200, Loss: 0.696921169757843\n",
      "Epoch 173/200, Loss: 0.8353623151779175\n",
      "Epoch 174/200, Loss: 0.7985082864761353\n",
      "Epoch 175/200, Loss: 0.8241244554519653\n",
      "Epoch 176/200, Loss: 0.643042266368866\n",
      "Epoch 177/200, Loss: 0.5940769910812378\n",
      "Epoch 178/200, Loss: 0.8315895795822144\n",
      "Epoch 179/200, Loss: 0.6803559064865112\n",
      "Epoch 180/200, Loss: 0.7629984617233276\n",
      "Epoch 181/200, Loss: 0.7780628204345703\n",
      "Epoch 182/200, Loss: 0.7143629193305969\n",
      "Epoch 183/200, Loss: 0.6929527521133423\n",
      "Epoch 184/200, Loss: 0.7669159173965454\n",
      "Epoch 185/200, Loss: 0.7486122846603394\n",
      "Epoch 186/200, Loss: 0.7355619668960571\n",
      "Epoch 187/200, Loss: 0.5840016603469849\n",
      "Epoch 188/200, Loss: 0.7226037383079529\n",
      "Epoch 189/200, Loss: 0.5597192645072937\n",
      "Epoch 190/200, Loss: 0.6111829876899719\n",
      "Epoch 191/200, Loss: 0.7051676511764526\n",
      "Epoch 192/200, Loss: 0.6689480543136597\n",
      "Epoch 193/200, Loss: 0.7533637285232544\n",
      "Epoch 194/200, Loss: 0.709254264831543\n",
      "Epoch 195/200, Loss: 0.7123339176177979\n",
      "Epoch 196/200, Loss: 0.7122227549552917\n",
      "Epoch 197/200, Loss: 0.8984530568122864\n",
      "Epoch 198/200, Loss: 0.6457377672195435\n",
      "Epoch 199/200, Loss: 0.8924519419670105\n",
      "Epoch 200/200, Loss: 0.7951771020889282\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # forward pass\n",
    "    y_pred = model3(torch.randn(10, 5))\n",
    "    y_true = torch.randint(0, 2, (10, 1)).float()\n",
    "    \n",
    "    # calculating the loss\n",
    "    # loss = model2.loss_function(y_pred, y_true)\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # updating the weights and biases\n",
    "    with torch.no_grad():\n",
    "        model3.model[0].weight -= lr * model3.model[0].weight.grad\n",
    "        model3.model[0].bias -= lr * model3.model[0].bias.grad\n",
    "    \n",
    "    # zero the gradients\n",
    "    model3.model[0].weight.grad.zero_()\n",
    "    model3.model[0].bias.grad.zero_()\n",
    "    \n",
    "    # print the loss\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using buildin optimization in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# defining optimization\n",
    "optimizer = torch.optim.SGD(model3.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    y_pred = model3(torch.randn(10, 5))\n",
    "    \n",
    "    # calculating loss\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "    \n",
    "    # clearing previous gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # performing backward pass and calculating gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # updating the weights and biases\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
